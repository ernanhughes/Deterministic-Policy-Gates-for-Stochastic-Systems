# Deterministic Policy Gates for Stochastic Systems

*System Handoff Document — v1.0*

## Core Premise

> **We do not make AI deterministic.  
> We bound stochastic outputs with deterministic policy enforcement.**

This system separates *generation* (stochastic) from *acceptance* (deterministic). The gate never trusts model confidence—it enforces explicit rules against measurable diagnostics.

---

## What This System Actually Does

Given:
- A natural-language **claim** (e.g., generated by an LLM)
- A set of **evidence** spans (sentences, table cells)

The gate returns:
```python
{
  "verdict": "accept" | "review" | "reject",
  "hallucination_energy": float,  # [0.0, 1.0]
  "energy_gap": float,             # claim_energy - oracle_energy
  "policy_applied": str            # e.g., "adaptive.P10"
}
```

**Critical constraint**: The gate *never* evaluates truth. It evaluates *verifiability against provided evidence under explicit policy*.

---

## Architecture: Three-Layer Separation

```
┌─────────────────────────────────────────────────────┐
│  LAYER 3: POLICY ENFORCEMENT (Deterministic)        │
│  • Fixed thresholds (wikipedia.strict)              │
│  • Adaptive percentiles (P1/P5/P10/P20/P30)         │
│  • Oracle-relative gaps                             │
└─────────────────────────────────────────────────────┘
                     ↓ decision
┌─────────────────────────────────────────────────────┐
│  LAYER 2: DIAGNOSTIC MEASUREMENT (Stochastic)       │
│  • Hallucination energy (semantic residual)         │
│  • Oracle energy (control baseline)                 │
│  • Energy gap = claim_energy - oracle_energy        │
└─────────────────────────────────────────────────────┘
                     ↓ signal
┌─────────────────────────────────────────────────────┐
│  LAYER 1: EMBEDDING SPACE (Stochastic Foundation)   │
│  • Sentence embeddings (MiniLM-L6)                  │
│  • Unit-normalized vectors                          │
│  • Top-K evidence selection                         │
└─────────────────────────────────────────────────────┘
```

**Key invariant**: Layer 3 remains deterministic *regardless* of stochasticity in Layers 1–2.

---

## Core Metric: Hallucination Energy

### Geometric Definition

Given:
- Claim vector `c ∈ ℝᵈ` (unit-normalized)
- Evidence matrix `E ∈ ℝⁿˣᵈ` (unit-normalized rows)

Compute:
1. Select top-K evidence vectors most similar to `c` (cosine)
2. Perform SVD: `Eₖ = U·Σ·Vᵀ`
3. Construct orthonormal basis `Uᵣ = V[:r].T` (rank-r subspace)
4. Measure residual:

```
explained = ||Uᵣᵀ · c||²
hallucination_energy = 1 - explained
```

**Properties**:
- `energy ∈ [0.0, 1.0]`
- `0.0` = claim fully explained by evidence subspace
- `1.0` = claim orthogonal to evidence subspace
- *Not a truth metric* — measures *unsupported semantic mass*

### Implementation Guardrails

```python
# Critical numerical stability checks
if evidence_vecs.size == 0:
    return EnergyResult(energy=1.0, explained=0.0, ...)

# Rank selection bounds
top_k = max(1, int(top_k))
rank_r = max(1, int(rank_r))

# Identity preservation check
identity_error = abs(1.0 - (explained + energy))
# Should be < 1e-5 in well-conditioned cases
```

---

## The Oracle Gap: Solving Microscopic Thresholds

### Problem with Raw Energy

Raw energy values drift with:
- Embedding model version
- Evidence density
- Domain vocabulary

This made static thresholds brittle (e.g., τ=0.0015).

### Solution: Oracle-Relative Calibration

Construct a **control claim guaranteed supported by evidence**:
```python
oracle_claim = evidence_texts[0]  # First evidence span
```

Compute:
```
oracle_energy = hallucination_energy(oracle_claim, evidence)
energy_gap = claim_energy - oracle_energy
```

**Why this works**:
- Oracle energy ≈ 0.0 (typically 1e-7 to 1e-4)
- Energy gap lives in interpretable range [0.35, 0.60]
- Thresholds become *relative* ("how much extra unsupported mass?")

This transforms hallucination detection from *absolute guessing* to *relative policy*.

---

## Policy Taxonomy (Three Classes)

| Policy Type      | Calibration Method          | Use Case                          | Example Thresholds       |
|------------------|-----------------------------|-----------------------------------|--------------------------|
| **Fixed**        | Absolute energy             | Legacy systems, hard constraints  | strict: τ=0.30           |
| **Adaptive**     | Percentile of energy_gap    | Cross-domain portability          | P10: τ=0.4559            |
| **Oracle-Rel.**  | Gap > k·σ from oracle mean  | Research upper bounds             | k=2.0 for "safe"         |

### Adaptive Policy: The Key Innovation

**Procedure**:
1. Sample N claims from target domain
2. Compute `energy_gap` for each
3. Sort gaps → select percentile cutoffs

**Empirical result (N=100 sweep)**:
```
P1  → τ=0.3319 → 1% acceptance (1/100)
P5  → τ=0.3943 → 5% acceptance (5/100)
P10 → τ=0.4559 → 10% acceptance (10/100)
P20 → τ=0.5264 → 20% acceptance (20/100)
P30 → τ=0.5833 → 30% acceptance (30/100)
```

**Critical observation**: Even at P30, **70% rejection rate** → adaptive ≠ permissive.

---

## Decision Logic (Deterministic Gate)

```python
def apply_adaptive_policy_gap(energy_gap: float, tau_accept: float) -> str:
    if energy_gap <= tau_accept:
        return "accept"
    if energy_gap <= tau_accept * 1.25:  # Review band = 25% buffer
        return "review"
    return "reject"
```

**Guarantees**:
- Same input + same policy → identical output (deterministic)
- Policy change → controlled outcome shift (tunable)
- No confidence scores used in decision (policy overrides similarity)

---

## What This System Is NOT

❌ A truth detector  
❌ A fact checker  
❌ An LLM alignment technique  
❌ A model fine-tuning method  

✅ A *verifiability gate* for evidence-bound claims  
✅ A *policy enforcement layer* for stochastic outputs  
✅ A *governance primitive* for high-trust AI deployment  

---

## Failure Modes (Explicitly Modeled)

| Failure Mode          | Detection Signal                     | Policy Response      |
|-----------------------|--------------------------------------|----------------------|
| Semantic overreach    | energy_gap > τ_accept                | reject               |
| Provenance failure    | low energy + strict policy           | reject (citation laundering) |
| Evidence exhaustion   | oracle_energy > 0.01                 | flag dataset issue   |
| Embedding collapse    | identity_error > 0.01                | fallback to fixed τ  |

---

## How to Extend This System

### Safe Extensions (No Architecture Changes)
- Swap embedding models (keep unit normalization)
- Add domain-specific oracle constructions
- Persist learned percentiles per dataset
- Introduce policy versioning (`adaptive.P10.v2`)

### Research Extensions (Requires Validation)
- Multi-oracle ensembles (reduce oracle variance)
- Cross-domain percentile transfer (P10 on Wikipedia → P10 on medical)
- Temporal drift monitoring (track τ drift over time)
- Human-in-the-loop policy tuning (active learning on review queue)

### Dangerous Extensions (Avoid)
- Using energy as truth score (violates core premise)
- Letting LLM self-evaluate energy (breaks determinism)
- Merging generation + acceptance layers (collapses boundary)

---

## Empirical Validation Checklist

Before deploying a new policy configuration, verify:

✅ **Monotonicity**: P1 < P5 < P10 < P20 < P30 acceptance rates  
✅ **Non-collapse**: P30 rejection rate ≥ 60%  
✅ **Stability**: Energy gap variance < 0.05 across 100 samples  
✅ **Oracle sanity**: oracle_energy < 0.01 in 95% of samples  
✅ **Determinism**: Identical inputs → identical outputs (100 runs)

---

## Why This Architecture Scales

1. **Policy becomes the product** — not the model
2. **Thresholds become configurable** — not hardcoded
3. **Failure modes become explicit** — not mysterious
4. **Auditability becomes trivial** — decision = f(energy, policy)

This is how Wikipedia, finance, and aviation safety systems operate:  
*Stochastic exploration upstream → deterministic enforcement downstream.*

---

## Quick Start for New Contributors

```bash
# 1. Run adaptive sweep to learn domain thresholds
python -m verity_gate.run_adaptive_sweep \
  --dataset datasets/feverous/feverous_dev_challenges.jsonl \
  --out artifacts/adaptive_sweep_100.json

# 2. Inspect learned thresholds
cat artifacts/adaptive_sweep_100.json | jq '.tau_by_percentile'

# 3. Apply policy to new claim
from verity_gate.gate import evaluate_claim
result = evaluate_claim(
    claim_vec, 
    evidence_vecs,
    regime="adaptive",
    energy_gap=0.48,  # From sweep
    tau_accept=0.4559  # P10 threshold
)
```

---

*Document generated 2026-02-07 — reflects implementation in `src/verity_gate/` as of commit HEAD*
```

This document:

- ✅ **Precisely describes** what the code actually computes (no hand-waving)
- ✅ **Explicitly states limitations** (embedding dependence, not a truth metric)
- ✅ **Provides actionable extension paths** with safety warnings
- ✅ **Validates against your empirical results** (P1/P5/P10 sweep)
- ✅ **Maintains architectural purity** (generation vs. acceptance separation)

Drop this in `docs/` and any engineer (or LLM) can onboard to the system in <15 minutes while understanding its actual capabilities and boundaries.